{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. How does bagging reduce overfitting in decision trees?\n\nBagging reduces overfitting in decision trees by averaging predictions from multiple models trained on different subsets of the data. This reduces the variance of the individual\ndecision trees. Since decision trees tend to overfit by capturing noise in the training data, aggregating predictions from several trees helps smooth out these errors and makes the final\nmodel more robust to overfitting.\n\n# Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n    \nAdvantages:\nDecision Trees: Highly flexible, capture complex patterns, and are effective in both classification and regression tasks.\nOther Base Learners (e.g., linear models): May offer a balance between flexibility and interpretability, and be faster to train compared to complex models.\n\nDisadvantages:\nDecision Trees: Can be prone to overfitting if not properly pruned, especially for shallow trees.\nLinear models: May not perform well on complex, non-linear data because they assume a linear relationship between features and the target.\nThe choice of base learner impacts the tradeoff between model complexity and predictive power.\n\n# Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n                                                                  \nHigh Bias, Low Variance (e.g., Linear Models): If the base learner is simple (e.g., linear models), the ensemble will have higher bias but lower variance. This can lead to underfitting \n                                               if the data is complex.\nLow Bias, High Variance (e.g., Decision Trees): If the base learner is complex (e.g., decision trees), the ensemble will have lower bias but higher variance. Bagging helps reduce this \n                                                variance and the overall model becomes more stable.\nThe goal of bagging is to reduce variance without significantly increasing bias, especially when using high-variance base learners like decision trees.\n\n# Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n                                                                                \nYes, bagging can be used for both classification and regression tasks.\n\nClassification: For classification, the final prediction is typically made by majority voting. Each base learner casts a vote, and the class with the most votes is selected.\nRegression: For regression, the final prediction is the average of the predictions made by each base learner.\nThe difference lies in the way the final prediction is aggregated: majority voting for classification and averaging for regression.\n\n# Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n                                                                                                                   \nThe ensemble size in bagging determines how many base learners (models) will be aggregated to form the final prediction. A larger ensemble typically results in more stable and accurate \npredictions, as it reduces variance further. However, the performance improvement may plateau after a certain number of models, and increasing the ensemble size beyond that point might\nlead to diminishing returns.\n\nRule of thumb: The ensemble typically includes around 50 to 100 models. The optimal number depends on the dataset and computational resources. In practice, a moderate number of models \n               usually provides a good tradeoff between accuracy and computational efficiency.\n\n# Q6. Can you provide an example of a real-world application of bagging in machine learning?\n                                                                                                                   \nA real-world application of bagging is in spam email detection. Decision trees are often used as base learners in a bagging ensemble to classify whether an email is spam or not. \nThe ensemble of decision trees helps to improve accuracy by reducing the overfitting that a single decision tree might suffer from, making it more robust to the noisy and varied nature\nof email content.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}